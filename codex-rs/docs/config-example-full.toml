# ============================================================================
# Codex 完整配置示例 - 涵盖不同场景的 Profile 配置
# 配置文件位置: ~/.codex/config.toml
# ============================================================================

# ----------------------------------------------------------------------------
# 全局默认配置 (所有 profile 的基础值)
# ----------------------------------------------------------------------------

# 默认模型
model = "claude-sonnet-4-5"

# 默认模型提供商 (必须在 model_providers 或 ~/.codex/providers/ 中定义)
model_provider = "anthropic"

# 审批策略: "never" | "unless-trusted" | "on-failure" | "on-request"
approval_policy = "unless-trusted"

# 沙箱模式: "read-only" | "workspace-write" | "full-access"
sandbox_mode = "workspace-write"

# ----------------------------------------------------------------------------
# HTTP 超时全局默认值 (单位: 毫秒)
# ----------------------------------------------------------------------------

# TCP 连接超时 (30秒)
http_connect_timeout_ms = 30000

# HTTP 请求总超时 (10分钟)
http_request_timeout_ms = 600000

# SSE 流空闲超时 (5分钟)
stream_idle_timeout_ms = 300000

# ----------------------------------------------------------------------------
# 模型推理参数全局默认值
# ----------------------------------------------------------------------------

# 温度参数 (0.0-2.0)，越高越随机
temperature = 0.7

# Top-P 采样参数 (0.0-1.0)
top_p = 0.9

# 频率惩罚 (-2.0 到 2.0)
frequency_penalty = 0.0

# 存在惩罚 (-2.0 到 2.0)
presence_penalty = 0.0

# 推理努力程度: "low" | "medium" | "high" (仅 o1 系列模型)
# model_reasoning_effort = "medium"

# 推理摘要模式: "none" | "experimental"
# model_reasoning_summary = "none"

# 模型详细程度: "concise" | "normal" | "verbose"
# model_verbosity = "normal"

# ----------------------------------------------------------------------------
# 日志配置
# ----------------------------------------------------------------------------
[logging]
# 日志级别: "trace" | "debug" | "info" | "warn" | "error"
level = "info"

# 在日志中显示文件名和行号 (调试时有用)
location = false

# 在日志中显示模块路径 (调试时有用)
target = false

# 时区配置: "local" | "utc"
timezone = "local"

# 模块级别的日志配置 (可选，覆盖默认级别)
modules = [
    "codex_core=info",      # 核心逻辑
    "codex_tui=warn",       # TUI 界面
    "codex_mcp=debug",      # MCP 集成
]

# ----------------------------------------------------------------------------
# 历史记录配置
# ----------------------------------------------------------------------------
[history]
# 持久化策略: "save-all" | "none"
persistence = "save-all"

# 最大历史文件大小 (字节，可选)
# max_bytes = 10485760  # 10MB

# ----------------------------------------------------------------------------
# TUI 配置
# ----------------------------------------------------------------------------
[tui]
# 桌面通知: false | true | 自定义命令
notifications = false
# notifications = ["notify-send", "Codex", "任务完成"]

# ----------------------------------------------------------------------------
# Shell 环境策略
# ----------------------------------------------------------------------------
[shell_environment_policy]
# 继承环境变量: "all" | "core" | "none"
inherit = "all"

# 忽略默认排除规则 (默认排除 *KEY*, *TOKEN*)
ignore_default_excludes = false

# 排除特定环境变量 (支持通配符)
exclude = [
    "*SECRET*",
    "*PASSWORD*",
]

# 设置自定义环境变量
[shell_environment_policy.set]
# CUSTOM_VAR = "value"

# 仅包含特定环境变量 (如果设置，只传递这些变量)
# include_only = ["PATH", "HOME"]

# 是否使用 shell profile 执行命令
experimental_use_profile = false

# ----------------------------------------------------------------------------
# 默认激活的 Profile (可通过 -p 参数覆盖)
# ----------------------------------------------------------------------------
profile = "production"

# ============================================================================
# Profile 定义区域 - 不同使用场景的配置集合
# ============================================================================

# ----------------------------------------------------------------------------
# Profile 1: Development - 开发环境配置
# 特点: 快速反馈、宽松权限、详细日志
# 使用: codex -p development
# ----------------------------------------------------------------------------
[profiles.development]
model = "gpt-4o"
model_provider = "openai"
approval_policy = "never"           # 开发时不需要审批，加快速度
sandbox_mode = "full-access"        # 完全访问权限，方便调试
temperature = 0.8                   # 更高的创造性
http_connect_timeout_ms = 15000     # 15秒连接超时（开发环境快速失败）
http_request_timeout_ms = 180000    # 3分钟请求超时（开发任务通常较快）
stream_idle_timeout_ms = 60000      # 1分钟流超时

# ----------------------------------------------------------------------------
# Profile 2: Production - 生产环境配置
# 特点: 安全第一、长超时、严格审批
# 使用: codex -p production
# ----------------------------------------------------------------------------
[profiles.production]
model = "claude-sonnet-4-5"
model_provider = "anthropic"
approval_policy = "on-request"      # 生产环境需要明确批准
sandbox_mode = "read-only"          # 只读访问，防止意外修改
temperature = 0.2                   # 更保守的输出
http_connect_timeout_ms = 60000     # 1分钟连接超时（生产环境容忍慢网络）
http_request_timeout_ms = 1200000   # 20分钟请求超时（生产任务可能很长）
stream_idle_timeout_ms = 600000     # 10分钟流超时

# ----------------------------------------------------------------------------
# Profile 3: Fast - 快速查询配置
# 特点: 快速模型、短超时、自动化
# 使用: codex -p fast "快速问题"
# ----------------------------------------------------------------------------
[profiles.fast]
model = "gpt-4o-mini"               # 使用快速小模型
model_provider = "openai"
approval_policy = "never"           # 快速查询自动批准
sandbox_mode = "read-only"          # 只读足够
temperature = 0.5                   # 中等创造性
http_connect_timeout_ms = 5000      # 5秒连接超时（快速失败）
http_request_timeout_ms = 60000     # 1分钟请求超时
stream_idle_timeout_ms = 30000      # 30秒流超时

# ----------------------------------------------------------------------------
# Profile 4: Thorough - 深度分析配置
# 特点: 强大模型、长超时、详细输出
# 使用: codex -p thorough "全面分析代码库"
# ----------------------------------------------------------------------------
[profiles.thorough]
model = "o1"                        # 使用推理能力最强的模型
model_provider = "openai"
model_reasoning_effort = "high"     # 最高推理努力
approval_policy = "on-failure"      # 失败时才询问
sandbox_mode = "workspace-write"    # 允许写入工作区
temperature = 0.3                   # 较低温度保证精确性
http_connect_timeout_ms = 60000     # 1分钟连接超时
http_request_timeout_ms = 3600000   # 60分钟请求超时（深度分析可能很长）
stream_idle_timeout_ms = 1800000    # 30分钟流超时

# ----------------------------------------------------------------------------
# Profile 5: Local - 本地自托管模型配置
# 特点: 自托管模型、极长超时、网络友好
# 使用: codex -p local
# ----------------------------------------------------------------------------
[profiles.local]
model = "qwen-coder-plus"           # 本地部署的模型
model_provider = "self-hosted"      # 自托管提供商
approval_policy = "unless-trusted"
sandbox_mode = "workspace-write"
temperature = 0.7
http_connect_timeout_ms = 120000    # 2分钟连接超时（本地服务器启动慢）
http_request_timeout_ms = 3600000   # 60分钟请求超时（本地模型推理慢）
stream_idle_timeout_ms = 900000     # 15分钟流超时（本地模型生成慢）

# ----------------------------------------------------------------------------
# Profile 6: Review - 代码审查配置
# 特点: 审查模型、严格沙箱、详细反馈
# 使用: codex -p review "审查这个 PR"
# ----------------------------------------------------------------------------
[profiles.review]
model = "gpt-5-codex"               # 专门的审查模型
model_provider = "openai"
approval_policy = "on-request"      # 审查操作需要确认
sandbox_mode = "read-only"          # 审查只需要读权限
temperature = 0.1                   # 极低温度，保证一致性和严谨性
top_p = 0.95
http_connect_timeout_ms = 30000
http_request_timeout_ms = 900000    # 15分钟（审查可能需要深入分析）
stream_idle_timeout_ms = 300000     # 5分钟

# ----------------------------------------------------------------------------
# Profile 7: Debug - 调试配置
# 特点: 详细日志、完全访问、快速迭代
# 使用: codex -p debug
# ----------------------------------------------------------------------------
[profiles.debug]
model = "gpt-4o"
model_provider = "openai"
approval_policy = "never"           # 调试时快速迭代
sandbox_mode = "full-access"        # 需要完全访问权限
temperature = 0.5
http_connect_timeout_ms = 30000
http_request_timeout_ms = 600000
stream_idle_timeout_ms = 300000

# ----------------------------------------------------------------------------
# Profile 8: CI/CD - 持续集成配置
# 特点: 自动化、短超时、失败快速
# 使用: codex -p cicd (在 CI 环境中)
# ----------------------------------------------------------------------------
[profiles.cicd]
model = "gpt-4o"
model_provider = "openai"
approval_policy = "never"           # CI 环境全自动
sandbox_mode = "workspace-write"    # CI 需要写入权限
temperature = 0.5
http_connect_timeout_ms = 10000     # 10秒（CI 环境要求快速失败）
http_request_timeout_ms = 300000    # 5分钟（CI 任务时间限制）
stream_idle_timeout_ms = 120000     # 2分钟

# ----------------------------------------------------------------------------
# Profile 9: OpenAI - OpenAI 优化配置
# 特点: 针对 OpenAI API 优化的超时和参数
# 使用: codex -p openai
# ----------------------------------------------------------------------------
[profiles.openai]
model = "gpt-4o"
model_provider = "openai"
approval_policy = "unless-trusted"
sandbox_mode = "workspace-write"
temperature = 0.7
top_p = 1.0
frequency_penalty = 0.0
presence_penalty = 0.0
http_connect_timeout_ms = 20000     # OpenAI 通常响应快
http_request_timeout_ms = 300000    # 5分钟
stream_idle_timeout_ms = 120000     # 2分钟

# ----------------------------------------------------------------------------
# Profile 10: Anthropic - Anthropic 优化配置
# 特点: 针对 Claude API 优化的超时和参数
# 使用: codex -p anthropic
# ----------------------------------------------------------------------------
[profiles.anthropic]
model = "claude-sonnet-4-5"
model_provider = "anthropic"
approval_policy = "unless-trusted"
sandbox_mode = "workspace-write"
temperature = 0.7
top_p = 0.9
http_connect_timeout_ms = 30000     # Anthropic 可能需要稍长连接时间
http_request_timeout_ms = 600000    # 10分钟
stream_idle_timeout_ms = 300000     # 5分钟

# ----------------------------------------------------------------------------
# Profile 11: Gateway-Fast - 快速网关配置
# 特点: 针对快速网关服务（如国内中转）
# 使用: codex -p gateway-fast
# ----------------------------------------------------------------------------
[profiles.gateway-fast]
model = "gpt-4o"
model_provider = "fast-gateway"     # 需要在 ~/.codex/providers/ 中定义
approval_policy = "unless-trusted"
sandbox_mode = "workspace-write"
temperature = 0.7
http_connect_timeout_ms = 10000     # 快速网关连接快
http_request_timeout_ms = 180000    # 3分钟（快速网关处理快）
stream_idle_timeout_ms = 60000      # 1分钟

# ----------------------------------------------------------------------------
# Profile 12: Gateway-Slow - 慢速网关配置
# 特点: 针对慢速网关服务（如免费代理）
# 使用: codex -p gateway-slow
# ----------------------------------------------------------------------------
[profiles.gateway-slow]
model = "gpt-4o"
model_provider = "slow-gateway"     # 需要在 ~/.codex/providers/ 中定义
approval_policy = "unless-trusted"
sandbox_mode = "workspace-write"
temperature = 0.7
http_connect_timeout_ms = 90000     # 慢速网关需要更长连接时间
http_request_timeout_ms = 1800000   # 30分钟
stream_idle_timeout_ms = 900000     # 15分钟

# ============================================================================
# 模型提供商定义 (也可以在 ~/.codex/providers/*.toml 中分别定义)
# ============================================================================

# 注意：以下 provider 定义仅为示例，实际使用时需要配置真实的 API 地址和认证

# [[model_providers]]
# name = "self-hosted"
# display_name = "Self-Hosted Qwen"
# base_url = "http://localhost:8080/v1"
# adapter = "gpt_openapi"
# # 针对本地慢速模型的超时配置
# request_timeout_ms = 3600000      # 60分钟
# stream_idle_timeout_ms = 900000   # 15分钟
#
# [[model_providers]]
# name = "fast-gateway"
# display_name = "Fast Gateway"
# base_url = "https://fast-api.example.com/v1"
# adapter = "gpt_openapi"
# # 快速网关的超时配置
# request_timeout_ms = 180000       # 3分钟
# stream_idle_timeout_ms = 60000    # 1分钟
#
# [[model_providers]]
# name = "slow-gateway"
# display_name = "Slow Gateway (Free)"
# base_url = "https://slow-api.example.com/v1"
# adapter = "gpt_openapi"
# # 慢速网关的超时配置
# request_timeout_ms = 1800000      # 30分钟
# stream_idle_timeout_ms = 900000   # 15分钟

# ============================================================================
# 使用示例
# ============================================================================

# 1. 使用默认配置 (production profile)
#    $ codex

# 2. 使用特定 profile
#    $ codex -p development "帮我实现功能X"
#    $ codex -p fast "这个函数做什么？"
#    $ codex -p thorough "全面分析这个项目的架构"
#    $ codex -p review "审查这个 PR"

# 3. 临时覆盖 profile 的某些参数
#    $ codex -p development -m gpt-4o "使用开发配置，但强制用 gpt-4o"
#    $ codex -p production -a never "使用生产配置，但不需要审批"

# 4. 在不同网络环境下切换
#    # 公司内网（快速）
#    $ codex -p openai
#
#    # 家庭网络（通过网关）
#    $ codex -p gateway-fast
#
#    # 移动网络（慢速）
#    $ codex -p gateway-slow

# 5. CI/CD 集成
#    # .github/workflows/codex.yml
#    - run: codex -p cicd "分析代码质量"

# ============================================================================
# 配置优先级总结
# ============================================================================

# 1. 命令行参数（最高优先级）
#    例如: -m, -p, -a, --sandbox
#
# 2. Profile 配置
#    例如: [profiles.development] 中的设置
#
# 3. 全局配置
#    例如: 文件顶部的 model, http_request_timeout_ms 等
#
# 4. 环境变量（仅部分参数）
#    例如: CODEX_HTTP_CONNECT_TIMEOUT_MS, CODEX_HTTP_REQUEST_TIMEOUT_MS
#
# 5. 硬编码默认值（最低优先级）
#    例如: http_connect_timeout_ms = 30000 (30秒)

# ============================================================================
# 调试技巧
# ============================================================================

# 1. 查看当前有效配置
#    $ RUST_LOG=debug codex
#    # 查看日志中的 "Using configuration" 信息

# 2. 测试不同超时设置
#    # 使用环境变量快速测试
#    $ CODEX_HTTP_REQUEST_TIMEOUT_MS=60000 codex -p fast

# 3. 日志调试
#    # 启用详细日志
#    $ RUST_LOG=codex_core=debug,codex_mcp=trace codex

# 4. 验证 profile 是否存在
#    # 如果 profile 不存在会报错: "config profile `xxx` not found"

# ============================================================================
# 更多配置选项
# ============================================================================

# 完整的配置字段说明请参考:
# - docs/timeout-configuration.md - HTTP 超时配置详解
# - docs/config-reference.md - 完整配置字段参考（如果存在）
# - CLAUDE.md - 开发者指南

# ============================================================================
