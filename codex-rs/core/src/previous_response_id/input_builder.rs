use crate::codex::Session;
use crate::codex::TurnContext;
use crate::protocol::EventMsg;
use crate::protocol::IncrementalInputUsedEvent;
use codex_protocol::models::ResponseItem;
use std::sync::Arc;

/// Determine if a ResponseItem was generated by the LLM (server already has it via previous_response_id).
///
/// # Algorithm
///
/// Items from the LLM have an `id` field populated by the server:
/// - Message (role=assistant), Reasoning, FunctionCall, CustomToolCall, LocalShellCall, WebSearchCall
///
/// Items that are user inputs (server needs them in the request):
/// - FunctionCallOutput, CustomToolCallOutput, Message (role=user), GhostSnapshot
///
/// This is a structural distinction - LLM outputs have `id: Option<String>` fields,
/// user inputs have only `call_id` or no ID field.
fn is_llm_generated(item: &ResponseItem) -> bool {
    match item {
        // LLM outputs (server has via previous_response_id)
        ResponseItem::Message { role, .. } if role == "assistant" => true,
        ResponseItem::Reasoning { .. } => true,
        ResponseItem::FunctionCall { .. } => true,
        ResponseItem::CustomToolCall { .. } => true,
        ResponseItem::LocalShellCall { .. } => true,
        ResponseItem::WebSearchCall { .. } => true,

        // User inputs (server needs in request)
        ResponseItem::FunctionCallOutput { .. } => false,
        ResponseItem::CustomToolCallOutput { .. } => false,
        ResponseItem::Message { role, .. } if role == "user" => false,
        ResponseItem::GhostSnapshot { .. } => false,
        ResponseItem::Other => false,

        // Edge case: assistant message without id (should not happen, but defensive)
        ResponseItem::Message { .. } => false,
    }
}

/// Build input for the current turn, using incremental history if possible.
///
/// # Decision Logic (Stateless Filtering Approach)
///
/// Uses incremental input when adapter supports `previous_response_id`.
/// NO state tracking needed - filters history by item type:
///
/// 1. Find last assistant message in history (marks end of last LLM response)
/// 2. Return only USER INPUT items after that point (tool outputs, user messages)
/// 3. LLM-generated items are filtered out (server already has them)
///
/// When using incremental mode, pending_input is appended to the filtered history.
/// Otherwise, falls back to full history.
///
/// # Arguments
///
/// - `session` - Current session with state and history
/// - `turn_context` - Current turn context with ModelClient
/// - `pending_input` - User-submitted items during model execution
///
/// # Returns
///
/// Vec<ResponseItem> ready to be sent to the model
pub async fn build_turn_input(
    session: &Arc<Session>,
    turn_context: &Arc<TurnContext>,
    pending_input: &[ResponseItem],
) -> Vec<ResponseItem> {
    // Check if adapter supports previous_response_id (no lock needed)
    let adapter_supports_incremental = turn_context
        .client
        .provider()
        .adapter
        .as_ref()
        .and_then(|name| {
            crate::adapters::get_adapter(name)
                .ok()
                .map(|adapter| adapter.supports_previous_response_id())
        })
        .unwrap_or(false);

    tracing::debug!(
        "Input mode decision: adapter_supports={}, pending_items={}, mode={}",
        adapter_supports_incremental,
        pending_input.len(),
        if adapter_supports_incremental {
            "incremental"
        } else {
            "full"
        }
    );

    if adapter_supports_incremental {
        // Build incremental input (only user inputs after last LLM response)
        let mut incremental_input = build_incremental_input_filtered(session).await;

        // Append any pending input (tool outputs, user messages during execution)
        incremental_input.extend_from_slice(pending_input);

        let total_items = incremental_input.len();
        let pending_count = pending_input.len();

        // Validate: must have at least one item to send
        if total_items == 0 {
            tracing::warn!("Incremental mode produced empty input - falling back to full history");
            return session.clone_history().await.get_history_for_prompt();
        }

        tracing::debug!(
            "Built incremental input: {} total items ({} filtered + {} pending)",
            total_items,
            total_items - pending_count,
            pending_count
        );

        // Notify UI that we're using incremental mode (for debugging/transparency)
        session
            .send_event(
                turn_context.as_ref(),
                EventMsg::IncrementalInputUsed(IncrementalInputUsedEvent {
                    items_count: total_items as i64,
                }),
            )
            .await;

        incremental_input
    } else {
        // Standard full history fallback
        session.clone_history().await.get_history_for_prompt()
    }
}

/// Build incremental input using stateless filtering (no state tracking).
///
/// # Algorithm (Type-Based Filtering)
///
/// 1. Get full history from ContextManager
/// 2. Find the LAST LLM-generated item (assistant message, reasoning, function call, etc.)
/// 3. Return only USER INPUT items after that point (tool outputs, user messages)
/// 4. Filter out any remaining LLM items (defensive, should not happen in practice)
///
/// This approach is stateless - no need for history_len tracking or clear_last_response() calls.
/// It's self-correcting: always sends exactly what the server needs based on item types.
///
/// # Arguments
///
/// - `session` - Current session with history
///
/// # Returns
///
/// User input items (tool outputs, user messages) that server doesn't have yet
async fn build_incremental_input_filtered(session: &Arc<Session>) -> Vec<ResponseItem> {
    let history = session.clone_history().await.get_history_for_prompt();
    let total_history_len = history.len();

    tracing::debug!(
        "Building incremental input from history (total_items={})",
        total_history_len
    );

    // Find the last LLM-generated item (marks end of last response)
    let last_llm_idx = history.iter().rposition(|item| is_llm_generated(item));

    if let Some(idx) = last_llm_idx {
        // Log the last LLM item found for debugging
        if let Some(last_llm_item) = history.get(idx) {
            let item_type = get_item_type_name(last_llm_item);
            tracing::debug!(
                "Found last LLM-generated item at index {} (type={}, total_items={})",
                idx,
                item_type,
                total_history_len
            );
        }
    } else {
        tracing::debug!(
            "No LLM-generated items in history - first turn, returning full history (total_items={})",
            total_history_len
        );
    }

    let incremental: Vec<_> = match last_llm_idx {
        Some(idx) => {
            let items_after_last_llm = total_history_len.saturating_sub(idx + 1);
            tracing::debug!(
                "Filtering {} items after last LLM output (skip {} items)",
                items_after_last_llm,
                idx + 1
            );

            // Return items AFTER last LLM output, filtering out any remaining LLM items
            let result: Vec<_> = history
                .into_iter()
                .enumerate()
                .skip(idx + 1)
                .filter(|(i, item)| {
                    let is_llm = is_llm_generated(item);
                    if is_llm {
                        // Defensive: should not happen, log if it does
                        tracing::warn!(
                            "Filtering out LLM item at index {} (type={}) - unexpected after last LLM output",
                            i,
                            get_item_type_name(item)
                        );
                    }
                    !is_llm
                })
                .map(|(_, item)| item)
                .collect();

            // Log filtered result breakdown
            let filtered_out = items_after_last_llm.saturating_sub(result.len());
            if filtered_out > 0 {
                tracing::debug!(
                    "Filtered out {} LLM items, kept {} user input items",
                    filtered_out,
                    result.len()
                );
            }

            result
        }
        None => {
            // No LLM outputs in history yet - return full history (first turn)
            // This is the fallback for conversations that haven't received any responses yet
            history
        }
    };

    let filtered_count = incremental.len();

    tracing::debug!(
        "Incremental filtering complete: {} user input items (from {} total history items, last_llm_idx={:?})",
        filtered_count,
        total_history_len,
        last_llm_idx
    );

    incremental
}

/// Helper function to get human-readable item type name for logging
fn get_item_type_name(item: &ResponseItem) -> &'static str {
    match item {
        ResponseItem::Message { role, .. } if role == "assistant" => "Message(assistant)",
        ResponseItem::Message { role, .. } if role == "user" => "Message(user)",
        ResponseItem::Message { .. } => "Message(other)",
        ResponseItem::Reasoning { .. } => "Reasoning",
        ResponseItem::FunctionCall { .. } => "FunctionCall",
        ResponseItem::FunctionCallOutput { .. } => "FunctionCallOutput",
        ResponseItem::CustomToolCall { .. } => "CustomToolCall",
        ResponseItem::CustomToolCallOutput { .. } => "CustomToolCallOutput",
        ResponseItem::LocalShellCall { .. } => "LocalShellCall",
        ResponseItem::WebSearchCall { .. } => "WebSearchCall",
        ResponseItem::GhostSnapshot { .. } => "GhostSnapshot",
        ResponseItem::Other => "Other",
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use codex_protocol::models::ContentItem;
    use codex_protocol::models::FunctionCallOutputPayload;
    use codex_protocol::models::ResponseItem;
    use pretty_assertions::assert_eq;

    #[test]
    fn test_is_llm_generated_identifies_assistant_messages() {
        let assistant_msg = ResponseItem::Message {
            id: Some("msg_123".to_string()),
            role: "assistant".to_string(),
            content: vec![ContentItem::OutputText {
                text: "response".to_string(),
            }],
        };
        assert!(is_llm_generated(&assistant_msg));

        let user_msg = ResponseItem::Message {
            id: None,
            role: "user".to_string(),
            content: vec![ContentItem::InputText {
                text: "query".to_string(),
            }],
        };
        assert!(!is_llm_generated(&user_msg));
    }

    #[test]
    fn test_is_llm_generated_identifies_function_calls() {
        let function_call = ResponseItem::FunctionCall {
            id: Some("fc_123".to_string()),
            name: "read_file".to_string(),
            arguments: "{}".to_string(),
            call_id: "call_1".to_string(),
        };
        assert!(is_llm_generated(&function_call));

        let function_output = ResponseItem::FunctionCallOutput {
            call_id: "call_1".to_string(),
            output: FunctionCallOutputPayload {
                content: "output".to_string(),
                content_items: None,
                success: Some(true),
            },
        };
        assert!(!is_llm_generated(&function_output));
    }

    #[test]
    fn test_is_llm_generated_identifies_reasoning() {
        let reasoning = ResponseItem::Reasoning {
            id: "rs_123".to_string(),
            summary: vec![],
            content: None,
            encrypted_content: None,
        };
        assert!(is_llm_generated(&reasoning));
    }

    #[test]
    fn test_filtering_returns_only_user_inputs_after_last_llm_output() {
        let history = vec![
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![ContentItem::InputText {
                    text: "first message".to_string(),
                }],
            },
            ResponseItem::Message {
                id: Some("msg_1".to_string()),
                role: "assistant".to_string(),
                content: vec![ContentItem::OutputText {
                    text: "first response".to_string(),
                }],
            },
            ResponseItem::FunctionCall {
                id: Some("fc_1".to_string()),
                name: "read_file".to_string(),
                arguments: "{}".to_string(),
                call_id: "call_1".to_string(),
            },
            ResponseItem::FunctionCallOutput {
                call_id: "call_1".to_string(),
                output: FunctionCallOutputPayload {
                    content: "file content".to_string(),
                    content_items: None,
                    success: Some(true),
                },
            },
        ];

        // Last LLM item is at index 2 (FunctionCall)
        // Should return only index 3 (FunctionCallOutput)
        let last_llm_idx = history.iter().rposition(|item| is_llm_generated(item));
        assert_eq!(last_llm_idx, Some(2));

        let incremental: Vec<_> = history
            .into_iter()
            .skip(last_llm_idx.unwrap() + 1)
            .filter(|item| !is_llm_generated(item))
            .collect();

        assert_eq!(incremental.len(), 1);
        assert!(matches!(
            incremental[0],
            ResponseItem::FunctionCallOutput { .. }
        ));
    }

    #[test]
    fn test_filtering_handles_no_llm_outputs() {
        let history = vec![ResponseItem::Message {
            id: None,
            role: "user".to_string(),
            content: vec![ContentItem::InputText {
                text: "first message".to_string(),
            }],
        }];

        // No LLM outputs - should return full history
        let last_llm_idx = history.iter().rposition(|item| is_llm_generated(item));
        assert_eq!(last_llm_idx, None);

        let result: Vec<_> = history.into_iter().collect();
        assert_eq!(result.len(), 1);
        assert!(matches!(&result[0], ResponseItem::Message { role, .. } if role == "user"));
    }

    #[test]
    fn test_filtering_handles_multiple_tool_outputs() {
        let history = vec![
            ResponseItem::Message {
                id: None,
                role: "user".to_string(),
                content: vec![ContentItem::InputText {
                    text: "query".to_string(),
                }],
            },
            ResponseItem::Reasoning {
                id: "rs_1".to_string(),
                summary: vec![],
                content: None,
                encrypted_content: None,
            },
            ResponseItem::FunctionCall {
                id: Some("fc_1".to_string()),
                name: "tool_1".to_string(),
                arguments: "{}".to_string(),
                call_id: "call_1".to_string(),
            },
            ResponseItem::FunctionCall {
                id: Some("fc_2".to_string()),
                name: "tool_2".to_string(),
                arguments: "{}".to_string(),
                call_id: "call_2".to_string(),
            },
            ResponseItem::FunctionCallOutput {
                call_id: "call_1".to_string(),
                output: FunctionCallOutputPayload {
                    content: "output_1".to_string(),
                    content_items: None,
                    success: Some(true),
                },
            },
            ResponseItem::FunctionCallOutput {
                call_id: "call_2".to_string(),
                output: FunctionCallOutputPayload {
                    content: "output_2".to_string(),
                    content_items: None,
                    success: Some(true),
                },
            },
        ];

        // Last LLM item is at index 3 (second FunctionCall)
        // Should return indices 4, 5 (both FunctionCallOutputs)
        let last_llm_idx = history.iter().rposition(|item| is_llm_generated(item));
        assert_eq!(last_llm_idx, Some(3));

        let incremental: Vec<_> = history
            .into_iter()
            .skip(last_llm_idx.unwrap() + 1)
            .filter(|item| !is_llm_generated(item))
            .collect();

        assert_eq!(incremental.len(), 2);
        assert!(matches!(
            incremental[0],
            ResponseItem::FunctionCallOutput { .. }
        ));
        assert!(matches!(
            incremental[1],
            ResponseItem::FunctionCallOutput { .. }
        ));
    }
}
